{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from random import Random\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, InMemoryDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.BRICS import FindBRICSBonds\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "from rdkit import RDLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# attentive_fp fashion featurization\n",
    "# -------------------------------------\n",
    "def onehot_encoding(x, allowable_set):\n",
    "    if x not in allowable_set:\n",
    "        raise Exception(\"input {0} not in allowable set{1}:\".format(\n",
    "            x, allowable_set))\n",
    "    return [x == s for s in allowable_set]\n",
    "\n",
    "\n",
    "def onehot_encoding_unk(x, allowable_set):\n",
    "    \"\"\"Maps inputs not in the allowable set to the last element.\"\"\"\n",
    "    if x not in allowable_set:\n",
    "        x = allowable_set[-1]\n",
    "    return [x == s for s in allowable_set]\n",
    "\n",
    "\n",
    "def atom_attr(mol, explicit_H=False, use_chirality=True, pharmaco=True, scaffold=True):\n",
    "    if pharmaco:\n",
    "        mol = tag_pharmacophore(mol)\n",
    "    if scaffold:\n",
    "        mol = tag_scaffold(mol)\n",
    "\n",
    "    feat = []\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        results = onehot_encoding_unk(\n",
    "            atom.GetSymbol(),\n",
    "            ['B', 'C', 'N', 'O', 'F', 'Si', 'P', 'S', 'Cl', 'As', 'Se', 'Br', 'Te', 'I', 'At', 'other'\n",
    "             ]) + onehot_encoding_unk(atom.GetDegree(),\n",
    "                                      [0, 1, 2, 3, 4, 5, 'other']) + \\\n",
    "                  [atom.GetFormalCharge(), atom.GetNumRadicalElectrons()] + \\\n",
    "                  onehot_encoding_unk(atom.GetHybridization(), [\n",
    "                      Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2,\n",
    "                      Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D,\n",
    "                      Chem.rdchem.HybridizationType.SP3D2, 'other'\n",
    "                  ]) + [atom.GetIsAromatic()]\n",
    "        if not explicit_H:\n",
    "            results = results + onehot_encoding_unk(atom.GetTotalNumHs(),\n",
    "                                                    [0, 1, 2, 3, 4])\n",
    "        if use_chirality:\n",
    "            try:\n",
    "                results = results + onehot_encoding_unk(\n",
    "                    atom.GetProp('_CIPCode'),\n",
    "                    ['R', 'S']) + [atom.HasProp('_ChiralityPossible')]\n",
    "            # print(one_of_k_encoding_unk(atom.GetProp('_CIPCode'), ['R', 'S']) + [atom.HasProp('_ChiralityPossible')])\n",
    "            except:\n",
    "                results = results + [0, 0] + [atom.HasProp('_ChiralityPossible')]\n",
    "        if pharmaco:\n",
    "            results = results + [int(atom.GetProp('Hbond_donor'))] + [int(atom.GetProp('Hbond_acceptor'))] + \\\n",
    "                      [int(atom.GetProp('Basic'))] + [int(atom.GetProp('Acid'))] + \\\n",
    "                      [int(atom.GetProp('Halogen'))]\n",
    "        if scaffold:\n",
    "            results = results + [int(atom.GetProp('Scaffold'))]\n",
    "        feat.append(results)\n",
    "\n",
    "    return np.array(feat)\n",
    "\n",
    "\n",
    "def bond_attr(mol, use_chirality=True):\n",
    "    feat = []\n",
    "    index = []\n",
    "    n = mol.GetNumAtoms()\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                bond = mol.GetBondBetweenAtoms(i, j)\n",
    "                if bond is not None:\n",
    "                    bt = bond.GetBondType()\n",
    "                    bond_feats = [\n",
    "                        bt == Chem.rdchem.BondType.SINGLE, bt == Chem.rdchem.BondType.DOUBLE,\n",
    "                        bt == Chem.rdchem.BondType.TRIPLE, bt == Chem.rdchem.BondType.AROMATIC,\n",
    "                        bond.GetIsConjugated(),\n",
    "                        bond.IsInRing()\n",
    "                    ]\n",
    "                    if use_chirality:\n",
    "                        bond_feats = bond_feats + onehot_encoding_unk(\n",
    "                            str(bond.GetStereo()),\n",
    "                            [\"STEREONONE\", \"STEREOANY\", \"STEREOZ\", \"STEREOE\"])\n",
    "                    feat.append(bond_feats)\n",
    "                    index.append([i, j])\n",
    "\n",
    "    return np.array(index), np.array(feat)\n",
    "\n",
    "\n",
    "def bond_break(mol):\n",
    "    results = np.array(sorted(list(FindBRICSBonds(mol))), dtype=np.long)\n",
    "\n",
    "    if results.size == 0:\n",
    "        cluster_idx = []\n",
    "        Chem.rdmolops.GetMolFrags(mol, asMols=True, frags=cluster_idx)\n",
    "        fra_edge_index, fra_edge_attr = bond_attr(mol)\n",
    "\n",
    "    else:\n",
    "        bond_to_break = results[:, 0, :]\n",
    "        bond_to_break = bond_to_break.tolist()\n",
    "        with Chem.RWMol(mol) as rwmol:\n",
    "            for i in bond_to_break:\n",
    "                rwmol.RemoveBond(*i)\n",
    "        rwmol = rwmol.GetMol()\n",
    "        cluster_idx = []\n",
    "        Chem.rdmolops.GetMolFrags(rwmol, asMols=True, sanitizeFrags=False, frags=cluster_idx)\n",
    "        fra_edge_index, fra_edge_attr = bond_attr(rwmol)\n",
    "        cluster_idx = torch.LongTensor(cluster_idx)\n",
    "\n",
    "    return fra_edge_index, fra_edge_attr, cluster_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------\n",
    "# Scaffold and pharmacophore information utils\n",
    "# ---------------------------------------------\n",
    "# tag pharmoco features to each atom\n",
    "fun_smarts = {\n",
    "        'Hbond_donor': '[$([N;!H0;v3,v4&+1]),$([O,S;H1;+0]),n&H1&+0]',\n",
    "        'Hbond_acceptor': '[$([O,S;H1;v2;!$(*-*=[O,N,P,S])]),$([O,S;H0;v2]),$([O,S;-]),$([N;v3;!$(N-*=[O,N,P,S])]),n&X2&H0&+0,$([o,s;+0;!$([o,s]:n);!$([o,s]:c:n)])]',\n",
    "        'Basic': '[#7;+,$([N;H2&+0][$([C,a]);!$([C,a](=O))]),$([N;H1&+0]([$([C,a]);!$([C,a](=O))])[$([C,a]);!$([C,a](=O))]),$([N;H0&+0]([C;!$(C(=O))])([C;!$(C(=O))])[C;!$(C(=O))]),$([n;X2;+0;-0])]',\n",
    "        'Acid': '[C,S](=[O,S,P])-[O;H1,-1]',\n",
    "        'Halogen': '[F,Cl,Br,I]'\n",
    "        }\n",
    "FunQuery = dict([(pharmaco, Chem.MolFromSmarts(s)) for (pharmaco, s) in fun_smarts.items()])\n",
    "\n",
    "\n",
    "def tag_pharmacophore(mol):\n",
    "    for fungrp, qmol in FunQuery.items():\n",
    "        matches = mol.GetSubstructMatches(qmol)\n",
    "        match_idxes = []\n",
    "        for mat in matches:\n",
    "            match_idxes.extend(mat)\n",
    "        for i, atom in enumerate(mol.GetAtoms()):\n",
    "            tag = '1' if i in match_idxes else '0'\n",
    "            atom.SetProp(fungrp, tag)\n",
    "    return mol\n",
    "\n",
    "\n",
    "# tag scaffold information to each atom\n",
    "def tag_scaffold(mol):\n",
    "    core = MurckoScaffold.GetScaffoldForMol(mol)\n",
    "    match_idxes = mol.GetSubstructMatch(core)\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        tag = '1' if i in match_idxes else '0'\n",
    "        atom.SetProp('Scaffold', tag)\n",
    "    return mol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# data and dataset\n",
    "# ---------------------------------\n",
    "class MolData(Data):\n",
    "    def __init__(self, fra_edge_index=None, fra_edge_attr=None, cluster_index=None, **kwargs):\n",
    "        super(MolData, self).__init__(**kwargs)\n",
    "        self.cluster_index = cluster_index\n",
    "        self.fra_edge_index = fra_edge_index\n",
    "        self.fra_edge_attr = fra_edge_attr\n",
    "\n",
    "    def __inc__(self, key, value, *args, **kwargs):\n",
    "        if key == 'cluster_index':\n",
    "            return int(self.cluster_index.max()) + 1\n",
    "        else:\n",
    "            return super().__inc__(key, value, *args, **kwargs)\n",
    "\n",
    "\n",
    "class MolDataset(InMemoryDataset):\n",
    "\n",
    "    def __init__(self, root, dataset, task_type, tasks, logger=None,\n",
    "                 transform=None, pre_transform=None, pre_filter=None):\n",
    "\n",
    "        self.tasks = tasks\n",
    "        self.dataset = dataset\n",
    "        self.task_type = task_type\n",
    "\n",
    "        super(MolDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['{}.csv'.format(self.dataset)]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['{}.pt'.format(self.dataset)]\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        df = pd.read_csv(self.raw_paths[0])\n",
    "        smilesList = df.smiles.values\n",
    "        print(f'number of all smiles: {len(smilesList)}')\n",
    "        remained_smiles = []\n",
    "        canonical_smiles_list = []\n",
    "        for smiles in smilesList:\n",
    "            try:\n",
    "                canonical_smiles_list.append(Chem.MolToSmiles(Chem.MolFromSmiles(smiles), isomericSmiles=True))\n",
    "                remained_smiles.append(smiles)\n",
    "            except:\n",
    "                print(f'not successfully processed smiles: {smiles}')\n",
    "                pass\n",
    "        print(f'number of successfully processed smiles: {len(remained_smiles)}')\n",
    "\n",
    "        df = df[df[\"smiles\"].isin(remained_smiles)].reset_index()\n",
    "        target = df[self.tasks].values\n",
    "        smilesList = df.smiles.values\n",
    "        data_list = []\n",
    "\n",
    "        for i, smi in enumerate(tqdm(smilesList)):\n",
    "\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            data = self.mol2graph(mol)\n",
    "\n",
    "            if data is not None:\n",
    "                label = target[i]\n",
    "                label[np.isnan(label)] = 666\n",
    "                data.y = torch.LongTensor([label])\n",
    "                if self.task_type == 'regression':\n",
    "                    data.y = torch.FloatTensor([label])\n",
    "                data_list.append(data)\n",
    "\n",
    "        if self.pre_filter is not None:\n",
    "            data_list = [data for data in data_list if self.pre_filter(data)]\n",
    "        if self.pre_transform is not None:\n",
    "            data_list = [self.pre_transform(data) for data in data_list]\n",
    "\n",
    "        data, slices = self.collate(data_list)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "\n",
    "    def mol2graph(self, mol):\n",
    "        smiles = Chem.MolToSmiles(mol)\n",
    "        if mol is None: return None\n",
    "        node_attr = atom_attr(mol)\n",
    "        edge_index, edge_attr = bond_attr(mol)\n",
    "        fra_edge_index, fra_edge_attr, cluster_index = bond_break(mol)\n",
    "        data = MolData(\n",
    "            x=torch.FloatTensor(node_attr),\n",
    "            edge_index=torch.LongTensor(edge_index).t(),\n",
    "            edge_attr=torch.FloatTensor(edge_attr),\n",
    "            fra_edge_index=torch.LongTensor(fra_edge_index).t(),\n",
    "            fra_edge_attr=torch.FloatTensor(fra_edge_attr),\n",
    "            cluster_index=torch.LongTensor(cluster_index),\n",
    "            y=None,\n",
    "            smiles=smiles,\n",
    "        )\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, Sequential, Parameter, Bilinear\n",
    "\n",
    "from torch_scatter import scatter\n",
    "from torch_geometric.nn import global_add_pool, GATConv\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn.inits import glorot, reset\n",
    "from torch_geometric.nn.pool.pool import pool_batch\n",
    "from torch_geometric.nn.pool.consecutive import consecutive_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Attention layers\n",
    "# ---------------------------------------\n",
    "class FeatureAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction):\n",
    "        super().__init__()\n",
    "        self.mlp = Sequential(\n",
    "            Linear(channels, channels // reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Linear(channels // reduction, channels, bias=False),\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        reset(self.mlp)\n",
    "\n",
    "    def forward(self, x, batch, size=None):\n",
    "        max_result = scatter(x, batch, dim=0, dim_size=size, reduce='max')\n",
    "        sum_result = scatter(x, batch, dim=0, dim_size=size, reduce='sum')\n",
    "        max_out = self.mlp(max_result)\n",
    "        sum_out = self.mlp(sum_result)\n",
    "        y = torch.sigmoid(max_out + sum_out)\n",
    "        y_ = y\n",
    "        y = y[batch]\n",
    "        return x * y, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# Neural tensor networks conv\n",
    "# ---------------------------------------\n",
    "class NTNConv(MessagePassing):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, slices, dropout, edge_dim=None, **kwargs):\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super(NTNConv, self).__init__(node_dim=0, **kwargs)\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.slices = slices\n",
    "        self.dropout = dropout\n",
    "        self.edge_dim = edge_dim\n",
    "\n",
    "        self.weight_node = Parameter(torch.Tensor(in_channels,\n",
    "                                                  out_channels))\n",
    "        if edge_dim is not None:\n",
    "            self.weight_edge = Parameter(torch.Tensor(edge_dim,\n",
    "                                                      out_channels))\n",
    "        else:\n",
    "            self.weight_edge = self.register_parameter('weight_edge', None)\n",
    "\n",
    "        self.bilinear = Bilinear(out_channels, out_channels, slices, bias=False)\n",
    "\n",
    "        if self.edge_dim is not None:\n",
    "            self.linear = Linear(3 * out_channels, slices)\n",
    "        else:\n",
    "            self.linear = Linear(2 * out_channels, slices)\n",
    "\n",
    "        self._alpha = None\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        glorot(self.weight_node)\n",
    "        glorot(self.weight_edge)\n",
    "        self.bilinear.reset_parameters()\n",
    "        self.linear.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr=None, return_attention_weights=None):\n",
    "\n",
    "        x = torch.matmul(x, self.weight_node)\n",
    "\n",
    "        if self.weight_edge is not None:\n",
    "            assert edge_attr is not None\n",
    "            edge_attr = torch.matmul(edge_attr, self.weight_edge)\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
    "\n",
    "        alpha = self._alpha\n",
    "        self._alpha = None\n",
    "\n",
    "        if isinstance(return_attention_weights, bool):\n",
    "            assert alpha is not None\n",
    "            return out, (edge_index, alpha)\n",
    "        else:\n",
    "            return out\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr):\n",
    "        score = self.bilinear(x_i, x_j)\n",
    "        if edge_attr is not None:\n",
    "            vec = torch.cat((x_i, edge_attr, x_j), 1)\n",
    "            block_score = self.linear(vec)  # bias already included\n",
    "        else:\n",
    "            vec = torch.cat((x_i, x_j), 1)\n",
    "            block_score = self.linear(vec)\n",
    "        scores = score + block_score\n",
    "        alpha = torch.tanh(scores)\n",
    "        self._alpha = alpha\n",
    "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
    "\n",
    "        dim_split = self.out_channels // self.slices\n",
    "        out = x_j.view(-1, self.slices, dim_split)\n",
    "\n",
    "        out = out * alpha.view(-1, self.slices, 1)\n",
    "        out = out.view(-1, self.out_channels)\n",
    "        return out\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, {}, slices={})'.format(self.__class__.__name__,\n",
    "                                              self.in_channels,\n",
    "                                              self.out_channels, self.slices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------\n",
    "# HiGNN backbone\n",
    "# ---------------------------------------\n",
    "class HiGNN(torch.nn.Module):\n",
    "    \"\"\"Hierarchical informative graph neural network for molecular representation.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, edge_dim, num_layers,\n",
    "                 slices, dropout, f_att=False, r=4, brics=True, cl=False):\n",
    "        super(HiGNN, self).__init__()\n",
    "\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.f_att = f_att\n",
    "        self.brics = brics\n",
    "        self.cl = cl\n",
    "\n",
    "        # atom feature transformation\n",
    "        self.lin_a = Linear(in_channels, hidden_channels)\n",
    "        self.lin_b = Linear(edge_dim, hidden_channels)\n",
    "\n",
    "        # convs block\n",
    "        self.atom_convs = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            conv = NTNConv(hidden_channels, hidden_channels, slices=slices,\n",
    "                           dropout=dropout, edge_dim=hidden_channels)\n",
    "            self.atom_convs.append(conv)\n",
    "\n",
    "        self.lin_gate = Linear(3 * hidden_channels, hidden_channels)\n",
    "\n",
    "        if self.f_att:\n",
    "            self.feature_att = FeatureAttention(channels=hidden_channels, reduction=r)\n",
    "\n",
    "        if self.brics:\n",
    "            # mol-fra attention\n",
    "            self.cross_att = GATConv(hidden_channels, hidden_channels, heads=4,\n",
    "                                     dropout=dropout, add_self_loops=False,\n",
    "                                     negative_slope=0.01, concat=False)\n",
    "\n",
    "        if self.brics:\n",
    "            self.out = Linear(2 * hidden_channels, out_channels)\n",
    "        else:\n",
    "            self.out = Linear(hidden_channels, out_channels)\n",
    "\n",
    "        if self.cl:\n",
    "            self.lin_project = Linear(hidden_channels, int(hidden_channels/2))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        self.lin_a.reset_parameters()\n",
    "        self.lin_b.reset_parameters()\n",
    "\n",
    "        for conv in self.atom_convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "        self.lin_gate.reset_parameters()\n",
    "\n",
    "        if self.f_att:\n",
    "            self.feature_att.reset_parameters()\n",
    "\n",
    "        if self.brics:\n",
    "            self.cross_att.reset_parameters()\n",
    "\n",
    "        self.out.reset_parameters()\n",
    "\n",
    "        if self.cl:\n",
    "            self.lin_project.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        # get mol input\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "        batch = data.batch\n",
    "\n",
    "        x = F.relu(self.lin_a(x))  # (N, 46) -> (N, hidden_channels)\n",
    "        edge_attr = F.relu(self.lin_b(edge_attr))  # (N, 10) -> (N, hidden_channels)\n",
    "\n",
    "        fa = []\n",
    "        # mol conv block\n",
    "        for i in range(0, self.num_layers):\n",
    "            h = F.relu(self.atom_convs[i](x, edge_index, edge_attr))\n",
    "            beta = self.lin_gate(torch.cat([x, h, x - h], 1)).sigmoid()\n",
    "            x = beta * x + (1 - beta) * h\n",
    "            if self.f_att:\n",
    "                x, y_ = self.feature_att(x, batch)\n",
    "                fa.append(y_)\n",
    "\n",
    "        mol_vec = global_add_pool(x, batch).relu_()\n",
    "\n",
    "        if self.brics:\n",
    "            # get fragment input\n",
    "            fra_x = data.x\n",
    "            fra_edge_index = data.fra_edge_index\n",
    "            fra_edge_attr = data.fra_edge_attr\n",
    "            cluster = data.cluster_index\n",
    "\n",
    "            fra_x = F.relu(self.lin_a(fra_x))  # (N, 46) -> (N, hidden_channels)\n",
    "            fra_edge_attr = F.leaky_relu_(self.lin_b(fra_edge_attr))  # (N, 10) -> (N, hidden_channels)\n",
    "\n",
    "            # fragment convs block\n",
    "            for i in range(0, self.num_layers):\n",
    "                fra_h = F.relu(self.atom_convs[i](fra_x, fra_edge_index, fra_edge_attr))\n",
    "                beta = self.lin_gate(torch.cat([fra_x, fra_h, fra_x - fra_h], 1)).sigmoid()\n",
    "                fra_x = beta * fra_x + (1 - beta) * fra_h\n",
    "                if self.f_att:\n",
    "                    fra_x, _ = self.feature_att(fra_x, cluster)\n",
    "\n",
    "            fra_x = global_add_pool(fra_x, cluster).relu_()\n",
    "\n",
    "            # get fragment batch\n",
    "            cluster, perm = consecutive_cluster(cluster)\n",
    "            fra_batch = pool_batch(perm, data.batch)\n",
    "\n",
    "            # molecule-fragment attention\n",
    "            row = torch.arange(fra_batch.size(0), device=batch.device)\n",
    "            mol_fra_index = torch.stack([row, fra_batch], dim=0)\n",
    "            fra_vec = self.cross_att((fra_x, mol_vec), mol_fra_index)\n",
    "            fra_vec = fra_vec.relu()\n",
    "\n",
    "            vectors_concat = list()\n",
    "            vectors_concat.append(mol_vec)\n",
    "            vectors_concat.append(fra_vec)\n",
    "\n",
    "            out = torch.cat(vectors_concat, 1)\n",
    "\n",
    "            # molecule-fragment contrastive\n",
    "            if self.cl:\n",
    "                out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "                return self.out(out), self.lin_project(mol_vec).relu_(), self.lin_project(fra_vec).relu_()\n",
    "            else:\n",
    "                out = F.dropout(out, p=self.dropout, training=self.training)\n",
    "                return self.out(out), fa\n",
    "\n",
    "        else:\n",
    "            assert self.cl is False\n",
    "            out = F.dropout(mol_vec, p=self.dropout, training=self.training)\n",
    "            return self.out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_best_result(model):\n",
    "    best_ckpt_path = 'E:/3-Code/Jupternote book/HiGNN_Vis/lipo_best_ckpt.pth'\n",
    "    ckpt = torch.load(best_ckpt_path, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(ckpt['model'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HiGNN(\n",
      "  (lin_a): Linear(in_features=46, out_features=256, bias=True)\n",
      "  (lin_b): Linear(in_features=10, out_features=256, bias=True)\n",
      "  (atom_convs): ModuleList(\n",
      "    (0): NTNConv(256, 256, slices=4)\n",
      "    (1): NTNConv(256, 256, slices=4)\n",
      "    (2): NTNConv(256, 256, slices=4)\n",
      "  )\n",
      "  (lin_gate): Linear(in_features=768, out_features=256, bias=True)\n",
      "  (feature_att): FeatureAttention(\n",
      "    (mlp): Sequential(\n",
      "      (0): Linear(in_features=256, out_features=256, bias=False)\n",
      "      (1): ReLU(inplace=True)\n",
      "      (2): Linear(in_features=256, out_features=256, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (cross_att): GATConv(256, 256, heads=4)\n",
      "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------\n",
    "# Build HiGNN \n",
    "# ---------------------------------------\n",
    "model = HiGNN(in_channels=46,\n",
    "              hidden_channels=256,\n",
    "              out_channels=1,\n",
    "              edge_dim=10,\n",
    "              num_layers=3,\n",
    "              dropout=0.1,\n",
    "              slices=4,\n",
    "              f_att=True,\n",
    "              r=1,\n",
    "              brics=True,\n",
    "              cl=False)\n",
    "\n",
    "model = load_best_result(model)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def get_header(path):\n",
    "    with open(path) as f:\n",
    "        header = next(csv.reader(f))\n",
    "\n",
    "    return header\n",
    "\n",
    "\n",
    "def get_task_names(path, use_compound_names=False):\n",
    "    index = 2 if use_compound_names else 1\n",
    "    task_names = get_header(path)[index:]\n",
    "\n",
    "    return task_names\n",
    "\n",
    "task_names = get_task_names('E:/3-Code/Jupternote book/HiGNN_Vis/raw/lipo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4200\n"
     ]
    }
   ],
   "source": [
    "path = 'E:/3-Code/Jupternote book/HiGNN_Vis'\n",
    "dataset = 'lipo'\n",
    "task_type = 'regression'\n",
    "tasks = task_names\n",
    "lipo = MolDataset(root=path, dataset=dataset, task_type=task_type, tasks=tasks)\n",
    "print(len(lipo))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "420"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 2021\n",
    "random = Random(seed)\n",
    "indices = list(range(len(lipo)))\n",
    "random.seed(seed)\n",
    "random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.8 * len(lipo))\n",
    "val_size = int(0.1 * len(lipo))\n",
    "test_size = len(lipo) - train_size - val_size\n",
    "\n",
    "trn_id, val_id, test_id = indices[:train_size], \\\n",
    "                          indices[train_size:(train_size + val_size)], \\\n",
    "                          indices[(train_size + val_size):]\n",
    "len(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipo_test = lipo[test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batch(batch=[11333], cluster_index=[11333], edge_attr=[24732, 10], edge_index=[2, 24732], fra_edge_attr=[20908, 10], fra_edge_index=[2, 20908], ptr=[421], smiles=[420], x=[11333, 46], y=[420, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader(lipo_test, batch_size=420)\n",
    "iter_ = iter(loader)\n",
    "batch = next(iter_)\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # 关闭dropout\n",
    "output = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4.5476],\n",
       "         [ 2.5327],\n",
       "         [-0.7744],\n",
       "         [ 2.3831],\n",
       "         [ 3.7411],\n",
       "         [-0.3080],\n",
       "         [-0.4591],\n",
       "         [ 2.3379],\n",
       "         [ 3.4778],\n",
       "         [ 3.4145],\n",
       "         [ 3.0135],\n",
       "         [ 1.5203],\n",
       "         [ 2.3754],\n",
       "         [ 2.9936],\n",
       "         [ 2.5694],\n",
       "         [ 3.1886],\n",
       "         [ 0.9795],\n",
       "         [ 3.2886],\n",
       "         [ 2.3403],\n",
       "         [ 3.4508],\n",
       "         [ 3.5335],\n",
       "         [ 3.7944],\n",
       "         [ 3.3515],\n",
       "         [-0.4806],\n",
       "         [ 2.2044],\n",
       "         [ 1.9528],\n",
       "         [ 2.4162],\n",
       "         [ 1.6023],\n",
       "         [ 3.4032],\n",
       "         [ 2.7480],\n",
       "         [ 3.3615],\n",
       "         [ 3.3672],\n",
       "         [ 2.6290],\n",
       "         [-0.8868],\n",
       "         [-0.3773],\n",
       "         [-0.6268],\n",
       "         [ 1.5146],\n",
       "         [-0.6786],\n",
       "         [ 2.8470],\n",
       "         [ 2.8245],\n",
       "         [ 1.6555],\n",
       "         [ 1.4006],\n",
       "         [-1.0928],\n",
       "         [ 1.9224],\n",
       "         [ 3.1041],\n",
       "         [ 2.3005],\n",
       "         [ 1.8578],\n",
       "         [ 2.4932],\n",
       "         [ 1.1916],\n",
       "         [ 4.0824],\n",
       "         [ 1.6917],\n",
       "         [ 1.1186],\n",
       "         [ 2.2026],\n",
       "         [ 1.4361],\n",
       "         [-0.9922],\n",
       "         [ 0.9763],\n",
       "         [ 0.6781],\n",
       "         [ 3.7718],\n",
       "         [ 4.0864],\n",
       "         [ 2.6098],\n",
       "         [ 2.1827],\n",
       "         [ 3.1859],\n",
       "         [ 1.5667],\n",
       "         [ 1.6562],\n",
       "         [ 3.0156],\n",
       "         [ 2.5723],\n",
       "         [ 3.1493],\n",
       "         [ 3.0359],\n",
       "         [ 4.0734],\n",
       "         [ 2.6205],\n",
       "         [ 2.6320],\n",
       "         [ 1.9448],\n",
       "         [-0.2651],\n",
       "         [ 2.2662],\n",
       "         [ 3.6951],\n",
       "         [ 1.9892],\n",
       "         [ 2.5233],\n",
       "         [ 2.8730],\n",
       "         [ 3.9270],\n",
       "         [ 3.4513],\n",
       "         [ 1.8202],\n",
       "         [ 2.2205],\n",
       "         [ 3.4563],\n",
       "         [ 1.6212],\n",
       "         [ 2.7084],\n",
       "         [ 2.9237],\n",
       "         [ 0.9606],\n",
       "         [ 2.4966],\n",
       "         [ 2.7853],\n",
       "         [ 2.8394],\n",
       "         [ 2.3941],\n",
       "         [ 2.8591],\n",
       "         [ 1.6630],\n",
       "         [ 2.2720],\n",
       "         [ 2.7325],\n",
       "         [ 2.8815],\n",
       "         [ 0.7685],\n",
       "         [ 0.3575],\n",
       "         [ 3.2590],\n",
       "         [ 3.4858],\n",
       "         [ 2.3000],\n",
       "         [ 2.3926],\n",
       "         [ 3.4514],\n",
       "         [ 0.6256],\n",
       "         [ 3.1626],\n",
       "         [ 3.5268],\n",
       "         [ 1.9823],\n",
       "         [ 2.5645],\n",
       "         [ 3.4033],\n",
       "         [ 1.1663],\n",
       "         [ 0.4106],\n",
       "         [-0.3224],\n",
       "         [ 2.6161],\n",
       "         [ 1.9855],\n",
       "         [ 2.6633],\n",
       "         [ 3.7956],\n",
       "         [ 3.1305],\n",
       "         [ 3.3433],\n",
       "         [ 2.4105],\n",
       "         [ 0.0175],\n",
       "         [ 2.9555],\n",
       "         [ 1.3989],\n",
       "         [ 2.4333],\n",
       "         [-1.0441],\n",
       "         [ 2.7945],\n",
       "         [ 2.1868],\n",
       "         [-0.6483],\n",
       "         [ 1.8658],\n",
       "         [ 1.1815],\n",
       "         [ 2.8361],\n",
       "         [ 3.0750],\n",
       "         [ 2.9072],\n",
       "         [ 3.4366],\n",
       "         [ 1.8820],\n",
       "         [ 2.5658],\n",
       "         [ 1.5914],\n",
       "         [ 3.4352],\n",
       "         [ 1.6970],\n",
       "         [ 3.6007],\n",
       "         [ 3.0223],\n",
       "         [ 2.0447],\n",
       "         [ 3.7106],\n",
       "         [ 1.5572],\n",
       "         [ 0.6812],\n",
       "         [ 1.2932],\n",
       "         [ 3.1918],\n",
       "         [ 2.7093],\n",
       "         [ 1.4855],\n",
       "         [ 0.6248],\n",
       "         [ 3.4338],\n",
       "         [ 3.0173],\n",
       "         [ 1.4029],\n",
       "         [ 0.5835],\n",
       "         [ 2.5963],\n",
       "         [ 2.6013],\n",
       "         [ 1.9766],\n",
       "         [ 1.6861],\n",
       "         [ 2.6435],\n",
       "         [ 2.8084],\n",
       "         [ 3.2727],\n",
       "         [ 2.0850],\n",
       "         [ 2.6642],\n",
       "         [ 1.3169],\n",
       "         [ 3.1229],\n",
       "         [ 2.3360],\n",
       "         [-0.1827],\n",
       "         [ 3.3010],\n",
       "         [ 3.1913],\n",
       "         [ 0.9816],\n",
       "         [ 2.3208],\n",
       "         [ 1.5226],\n",
       "         [ 1.9036],\n",
       "         [ 3.2754],\n",
       "         [ 2.4144],\n",
       "         [-0.1357],\n",
       "         [ 2.5148],\n",
       "         [ 1.7372],\n",
       "         [-0.3653],\n",
       "         [-0.9795],\n",
       "         [ 1.2335],\n",
       "         [ 3.5494],\n",
       "         [ 3.2286],\n",
       "         [ 2.7848],\n",
       "         [ 1.9996],\n",
       "         [ 2.2706],\n",
       "         [ 1.7918],\n",
       "         [-0.2579],\n",
       "         [ 1.4688],\n",
       "         [ 2.6154],\n",
       "         [ 1.9448],\n",
       "         [ 1.7305],\n",
       "         [ 1.7423],\n",
       "         [-0.9651],\n",
       "         [ 3.2420],\n",
       "         [ 1.7709],\n",
       "         [ 2.3577],\n",
       "         [ 0.5011],\n",
       "         [ 2.8957],\n",
       "         [ 2.3568],\n",
       "         [ 3.3406],\n",
       "         [ 0.1041],\n",
       "         [ 0.9401],\n",
       "         [ 0.8220],\n",
       "         [ 0.9453],\n",
       "         [ 3.0817],\n",
       "         [ 2.0656],\n",
       "         [ 1.1118],\n",
       "         [ 2.1208],\n",
       "         [ 1.5687],\n",
       "         [ 1.8088],\n",
       "         [ 2.6791],\n",
       "         [ 0.0886],\n",
       "         [ 2.2639],\n",
       "         [ 2.4856],\n",
       "         [ 2.2827],\n",
       "         [-0.0598],\n",
       "         [ 1.6524],\n",
       "         [ 2.3119],\n",
       "         [ 1.9396],\n",
       "         [ 2.4388],\n",
       "         [ 2.2476],\n",
       "         [ 4.2100],\n",
       "         [ 3.6285],\n",
       "         [ 2.9698],\n",
       "         [ 3.1448],\n",
       "         [ 3.0728],\n",
       "         [ 2.7004],\n",
       "         [ 3.2415],\n",
       "         [ 3.7692],\n",
       "         [ 2.7537],\n",
       "         [ 2.6633],\n",
       "         [ 1.4358],\n",
       "         [-0.8948],\n",
       "         [ 1.5330],\n",
       "         [ 1.4357],\n",
       "         [ 2.6817],\n",
       "         [ 0.1378],\n",
       "         [-0.5513],\n",
       "         [ 1.2286],\n",
       "         [-0.2068],\n",
       "         [ 2.9192],\n",
       "         [ 2.0847],\n",
       "         [ 1.6529],\n",
       "         [ 2.9140],\n",
       "         [ 3.0740],\n",
       "         [-1.0711],\n",
       "         [ 3.5401],\n",
       "         [ 0.5484],\n",
       "         [ 3.8675],\n",
       "         [ 0.8440],\n",
       "         [ 3.3384],\n",
       "         [-0.5183],\n",
       "         [ 3.0955],\n",
       "         [ 3.3332],\n",
       "         [ 3.0180],\n",
       "         [ 1.2894],\n",
       "         [ 2.6982],\n",
       "         [ 3.5085],\n",
       "         [-0.5431],\n",
       "         [ 2.6946],\n",
       "         [ 2.2614],\n",
       "         [ 2.5675],\n",
       "         [ 2.8097],\n",
       "         [ 2.0254],\n",
       "         [ 3.4722],\n",
       "         [ 1.3167],\n",
       "         [ 2.3508],\n",
       "         [ 2.9171],\n",
       "         [ 3.6118],\n",
       "         [ 2.5541],\n",
       "         [ 1.5824],\n",
       "         [ 2.3263],\n",
       "         [ 3.4162],\n",
       "         [ 3.3633],\n",
       "         [ 1.9759],\n",
       "         [ 1.0965],\n",
       "         [ 2.4856],\n",
       "         [ 3.4285],\n",
       "         [ 2.4953],\n",
       "         [ 2.9973],\n",
       "         [ 0.6077],\n",
       "         [ 2.8397],\n",
       "         [ 2.5740],\n",
       "         [ 4.0692],\n",
       "         [ 3.1199],\n",
       "         [ 1.9361],\n",
       "         [ 2.4326],\n",
       "         [ 2.2509],\n",
       "         [ 1.9222],\n",
       "         [ 0.8634],\n",
       "         [ 2.7379],\n",
       "         [ 0.6126],\n",
       "         [ 2.2909],\n",
       "         [ 2.1149],\n",
       "         [ 3.3771],\n",
       "         [ 1.7032],\n",
       "         [ 3.1103],\n",
       "         [ 1.6161],\n",
       "         [ 3.7102],\n",
       "         [ 3.3643],\n",
       "         [ 2.6781],\n",
       "         [ 2.8624],\n",
       "         [ 4.0329],\n",
       "         [ 2.3397],\n",
       "         [ 1.8867],\n",
       "         [ 3.6506],\n",
       "         [ 3.5470],\n",
       "         [ 3.2313],\n",
       "         [ 4.0589],\n",
       "         [ 2.7165],\n",
       "         [ 1.2438],\n",
       "         [ 3.0781],\n",
       "         [ 2.4539],\n",
       "         [ 3.3482],\n",
       "         [ 3.9092],\n",
       "         [ 1.3505],\n",
       "         [ 3.1139],\n",
       "         [ 2.5543],\n",
       "         [ 1.1858],\n",
       "         [ 2.8809],\n",
       "         [ 2.3556],\n",
       "         [ 3.0818],\n",
       "         [ 0.1366],\n",
       "         [ 2.2785],\n",
       "         [ 2.6197],\n",
       "         [ 2.7172],\n",
       "         [ 3.1610],\n",
       "         [ 1.2528],\n",
       "         [ 2.3932],\n",
       "         [ 4.0814],\n",
       "         [ 1.6365],\n",
       "         [ 2.8372],\n",
       "         [ 3.2106],\n",
       "         [ 2.8155],\n",
       "         [ 3.0662],\n",
       "         [ 1.7561],\n",
       "         [ 2.2392],\n",
       "         [ 1.8582],\n",
       "         [ 3.2568],\n",
       "         [-0.8849],\n",
       "         [ 3.2420],\n",
       "         [ 3.1003],\n",
       "         [ 2.5474],\n",
       "         [ 0.7680],\n",
       "         [ 2.1598],\n",
       "         [ 1.9443],\n",
       "         [ 0.0420],\n",
       "         [ 3.3374],\n",
       "         [ 3.6820],\n",
       "         [ 2.5225],\n",
       "         [ 1.4825],\n",
       "         [ 2.8232],\n",
       "         [ 2.0466],\n",
       "         [ 1.1856],\n",
       "         [ 1.8306],\n",
       "         [-0.9429],\n",
       "         [ 0.8167],\n",
       "         [ 3.3810],\n",
       "         [ 1.1891],\n",
       "         [ 3.7827],\n",
       "         [ 2.5743],\n",
       "         [ 0.7893],\n",
       "         [ 3.7240],\n",
       "         [ 2.6822],\n",
       "         [ 1.2634],\n",
       "         [ 3.7743],\n",
       "         [ 3.6563],\n",
       "         [ 0.9652],\n",
       "         [ 1.8156],\n",
       "         [ 3.0393],\n",
       "         [ 1.4455],\n",
       "         [ 1.1403],\n",
       "         [ 2.4535],\n",
       "         [ 3.0740],\n",
       "         [ 1.5424],\n",
       "         [ 3.6274],\n",
       "         [ 3.8230],\n",
       "         [ 3.6248],\n",
       "         [ 1.5447],\n",
       "         [ 2.4866],\n",
       "         [-1.0030],\n",
       "         [ 3.5400],\n",
       "         [ 3.2818],\n",
       "         [ 1.5440],\n",
       "         [ 2.1487],\n",
       "         [ 0.7328],\n",
       "         [ 3.1463],\n",
       "         [ 1.1143],\n",
       "         [ 2.8035],\n",
       "         [ 2.8502],\n",
       "         [ 3.4050],\n",
       "         [ 0.8368],\n",
       "         [ 3.0474],\n",
       "         [ 2.1846],\n",
       "         [ 4.1108],\n",
       "         [ 0.5880],\n",
       "         [ 1.3931],\n",
       "         [ 1.4453],\n",
       "         [ 2.7268],\n",
       "         [ 1.9997],\n",
       "         [ 2.8408],\n",
       "         [ 2.1158],\n",
       "         [ 3.6633],\n",
       "         [ 3.8535],\n",
       "         [ 1.1476],\n",
       "         [ 2.2976],\n",
       "         [ 1.4426],\n",
       "         [ 2.0846],\n",
       "         [ 3.6100],\n",
       "         [ 3.3522],\n",
       "         [ 0.9489],\n",
       "         [ 2.8845],\n",
       "         [-0.0728],\n",
       "         [ 2.7267],\n",
       "         [ 1.7448],\n",
       "         [ 1.0118],\n",
       "         [ 3.7865],\n",
       "         [ 3.8576],\n",
       "         [ 3.3967],\n",
       "         [ 0.7214]], grad_fn=<AddmmBackward>),\n",
       " [tensor([[9.9972e-01, 4.5765e-04, 5.4149e-01,  ..., 4.8502e-01, 9.9905e-01,\n",
       "           9.9956e-01],\n",
       "          [9.8388e-01, 2.7582e-03, 1.6685e-02,  ..., 1.5346e-01, 9.8854e-01,\n",
       "           9.9763e-01],\n",
       "          [1.5455e-01, 1.9945e-01, 1.3704e-02,  ..., 3.7220e-02, 6.0389e-01,\n",
       "           9.9441e-01],\n",
       "          ...,\n",
       "          [9.7159e-01, 1.0371e-02, 1.8416e-02,  ..., 2.8550e-01, 9.8122e-01,\n",
       "           9.9800e-01],\n",
       "          [9.3229e-01, 6.5714e-01, 2.7755e-02,  ..., 6.4404e-02, 9.3154e-01,\n",
       "           9.9930e-01],\n",
       "          [3.0307e-02, 7.6258e-01, 4.1645e-03,  ..., 1.9088e-02, 9.0959e-01,\n",
       "           9.9916e-01]], grad_fn=<SigmoidBackward>),\n",
       "  tensor([[0.6166, 0.0041, 0.1388,  ..., 0.4066, 0.9360, 0.9573],\n",
       "          [0.5461, 0.0152, 0.0659,  ..., 0.1073, 0.8633, 0.9873],\n",
       "          [0.0086, 0.3646, 0.0219,  ..., 0.0174, 0.3534, 0.9877],\n",
       "          ...,\n",
       "          [0.9576, 0.0069, 0.1366,  ..., 0.3969, 0.9203, 0.9441],\n",
       "          [0.9526, 0.0205, 0.1152,  ..., 0.3000, 0.9094, 0.9406],\n",
       "          [0.0177, 0.1410, 0.0278,  ..., 0.0354, 0.5433, 0.9876]],\n",
       "         grad_fn=<SigmoidBackward>),\n",
       "  tensor([[0.8151, 0.0338, 0.2891,  ..., 0.4862, 0.8098, 0.8080],\n",
       "          [0.2861, 0.2843, 0.1805,  ..., 0.2545, 0.5639, 0.8428],\n",
       "          [0.1001, 0.5143, 0.0825,  ..., 0.1716, 0.3765, 0.9228],\n",
       "          ...,\n",
       "          [0.6686, 0.1324, 0.2979,  ..., 0.4324, 0.6571, 0.7395],\n",
       "          [0.6023, 0.1461, 0.3250,  ..., 0.4407, 0.6819, 0.7432],\n",
       "          [0.0829, 0.4449, 0.1278,  ..., 0.1637, 0.4418, 0.8926]],\n",
       "         grad_fn=<SigmoidBackward>)])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = output[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = [i.detach().numpy() for i in fa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = [i.mean(0) for i in fa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.DataFrame(fa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = df_out.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.491677</td>\n",
       "      <td>0.592782</td>\n",
       "      <td>0.426811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.231353</td>\n",
       "      <td>0.065362</td>\n",
       "      <td>0.253433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.087736</td>\n",
       "      <td>0.112412</td>\n",
       "      <td>0.241528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.265183</td>\n",
       "      <td>0.331149</td>\n",
       "      <td>0.420120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.171920</td>\n",
       "      <td>0.135532</td>\n",
       "      <td>0.325843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.992288</td>\n",
       "      <td>0.955592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>0.996483</td>\n",
       "      <td>0.952863</td>\n",
       "      <td>0.834485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>0.153804</td>\n",
       "      <td>0.261173</td>\n",
       "      <td>0.349257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>0.933996</td>\n",
       "      <td>0.805340</td>\n",
       "      <td>0.620757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>0.995555</td>\n",
       "      <td>0.938912</td>\n",
       "      <td>0.792534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>256 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2\n",
       "0    0.491677  0.592782  0.426811\n",
       "1    0.231353  0.065362  0.253433\n",
       "2    0.087736  0.112412  0.241528\n",
       "3    0.265183  0.331149  0.420120\n",
       "4    0.171920  0.135532  0.325843\n",
       "..        ...       ...       ...\n",
       "251  0.997500  0.992288  0.955592\n",
       "252  0.996483  0.952863  0.834485\n",
       "253  0.153804  0.261173  0.349257\n",
       "254  0.933996  0.805340  0.620757\n",
       "255  0.995555  0.938912  0.792534\n",
       "\n",
       "[256 rows x 3 columns]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.to_csv('lipo_beta3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
